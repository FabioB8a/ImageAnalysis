{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final Tecnologías Emergentes\n",
    "\n",
    "### Integrantes\n",
    "    Fabio Buitrago\n",
    "    Luis Alejandro Bravo Ferreira\n",
    "    Ana María Ortegón Sepúlveda\n",
    "\n",
    "### Objetivo\n",
    "El objetivo del proyecto es resolver un problema contextualizado en sistemas de información geográfica realizando una clasificación de imágenes satelitales. Para esto se debe hacer uso del conjunto de imágenes adjunta a este proyecto, explorando los datos, codificando el modelo deseado y generando un reporte con los resultados de la solución construida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración y Descripción del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Analiza y resume los datos de imágenes en diferentes categorías.\n",
    "\n",
    "    La función itera sobre cada categoría especificada, accede a su correspondiente\n",
    "    directorio dentro del directorio base y procesa todas las imágenes contenidas en él.\n",
    "    Para cada imagen, calcula el promedio y desviación estándar de los colores, así como\n",
    "    su histograma de color. Cada detalle de imagen es agregado a un DataFrame para\n",
    "    posterior análisis. En caso de que no exista el directorio de alguna categoría, se\n",
    "    maneja la excepción correspondiente.\n",
    "\n",
    "    Args:\n",
    "    - base_dir (str): Directorio base que contiene las carpetas de cada categoría.\n",
    "    - categories (list of str): Lista de nombres de categorías para procesar.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Un diccionario que contiene un resumen de datos para cada categoría, incluyendo\n",
    "            un DataFrame con los detalles de las imágenes y el total de imágenes en esa categoría.\n",
    "\"\"\"\n",
    "def summarize_image_data(base_dir, categories):\n",
    "\n",
    "    data_summary = {}\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        try:\n",
    "            images = os.listdir(category_path)\n",
    "            image_details = []\n",
    "            for image_name in images:\n",
    "                image_path = os.path.join(category_path, image_name)\n",
    "                with Image.open(image_path) as img:\n",
    "                    img_array = np.array(img)\n",
    "                    mean_colors = np.mean(img_array, axis=(0, 1)) if img_array.ndim == 3 else np.mean(img_array)\n",
    "                    std_colors = np.std(img_array, axis=(0, 1)) if img_array.ndim == 3 else np.std(img_array)\n",
    "                    hist_colors = [np.histogram(img_array[:, :, i], bins=256, range=(0, 256))[0]\n",
    "                                   for i in range(img_array.shape[-1])] if img_array.ndim == 3 else np.histogram(img_array, bins=256, range=(0, 256))[0]\n",
    "                    image_details.append({\n",
    "                        'filename': image_name,\n",
    "                        'width': img.width,\n",
    "                        'height': img.height,\n",
    "                        'mode': img.mode,\n",
    "                        'mean_colors': mean_colors,\n",
    "                        'std_colors': std_colors,\n",
    "                        'histogram': hist_colors\n",
    "                    })\n",
    "            data_summary[category] = {\n",
    "                'count': len(images),\n",
    "                'details': pd.DataFrame(image_details)\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No se encontró la carpeta: {category_path}\")\n",
    "    return data_summary\n",
    "\n",
    "def show_sample_images_and_histograms(data_summary, base_dir, categories):\n",
    "    fig, axs = plt.subplots(2, len(categories), figsize=(20, 10), gridspec_kw={'height_ratios': [3, 1]})\n",
    "    for i, category in enumerate(categories):\n",
    "        if data_summary.get(category):\n",
    "            image_path = os.path.join(base_dir, category, data_summary[category]['details'].iloc[0]['filename'])\n",
    "            img = Image.open(image_path)\n",
    "            img_array = np.array(img)\n",
    "            axs[0, i].imshow(img)\n",
    "            axs[0, i].set_title(f\"{category} (Muestra)\")\n",
    "            axs[0, i].axis('off')\n",
    "\n",
    "            if img_array.ndim == 3:\n",
    "                for j in range(3):\n",
    "                    axs[1, i].plot(data_summary[category]['details'].iloc[0]['histogram'][j], color=['r', 'g', 'b'][j])\n",
    "            else:\n",
    "                axs[1, i].plot(data_summary[category]['details'].iloc[0]['histogram'], color='gray')\n",
    "    plt.show()\n",
    "\n",
    "base_dir = 'data'\n",
    "categories = ['cloudy', 'green_area', 'desert', 'water']\n",
    "\n",
    "data_summary = summarize_image_data(base_dir, categories)\n",
    "show_sample_images_and_histograms(data_summary, base_dir, categories)\n",
    "\n",
    "for category in categories:\n",
    "    if data_summary.get(category):\n",
    "        print(f\"Categoría: {category}\")\n",
    "        print(f\"  Número total de imágenes: {data_summary[category]['count']}\")\n",
    "        print(f\"  Media de colores: {data_summary[category]['details']['mean_colors'].mean()}\")\n",
    "        print(f\"  Desviación estándar de colores: {data_summary[category]['details']['std_colors'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Divide las imágenes de una categoría en conjuntos de entrenamiento y prueba,\n",
    "    y copia las imágenes correspondientes en los directorios respectivos.\n",
    "\n",
    "    Args:\n",
    "        category (str): El nombre de la categoría (subdirectorio) de las imágenes.\n",
    "        source_dir (str): La ruta del directorio fuente que contiene las imágenes.\n",
    "        train_dir (str): La ruta del directorio principal para las imágenes de entrenamiento.\n",
    "        test_dir (str): La ruta del directorio principal para las imágenes de prueba.\n",
    "        train_size (float, optional): Proporción de las imágenes que se utilizarán para el entrenamiento. \n",
    "                                      El resto se utilizará para la prueba. Por defecto es 0.8.\n",
    "\"\"\"\n",
    "def split_and_copy_images(category, source_dir, train_dir, test_dir, train_size=0.8):\n",
    "    images = os.listdir(source_dir)\n",
    "    train_images, test_images = train_test_split(images, train_size=train_size, random_state=42)\n",
    "    \n",
    "    category_train_dir = os.path.join(train_dir, category)\n",
    "    category_test_dir = os.path.join(test_dir, category)\n",
    "    \n",
    "    os.makedirs(category_train_dir, exist_ok=True)\n",
    "    os.makedirs(category_test_dir, exist_ok=True)\n",
    "    \n",
    "    for image in train_images:\n",
    "        src_path = os.path.join(source_dir, image)\n",
    "        dst_path = os.path.join(category_train_dir, image)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    for image in test_images:\n",
    "        src_path = os.path.join(source_dir, image)\n",
    "        dst_path = os.path.join(category_test_dir, image)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "\"\"\"\n",
    "    Prepara los generadores de datos de imágenes para entrenamiento y prueba.\n",
    "\n",
    "    Args:\n",
    "        cloudy_path (str): Ruta del directorio que contiene las imágenes de la categoría 'cloudy'.\n",
    "        desert_path (str): Ruta del directorio que contiene las imágenes de la categoría 'desert'.\n",
    "        green_path (str): Ruta del directorio que contiene las imágenes de la categoría 'green'.\n",
    "        water_path (str): Ruta del directorio que contiene las imágenes de la categoría 'water'.\n",
    "        base_dir (str, optional): Ruta base donde se crearán los directorios de entrenamiento y prueba. \n",
    "                                  Por defecto es './data/'.\n",
    "        train_size (float, optional): Proporción de las imágenes que se utilizarán para el entrenamiento. \n",
    "                                      El resto se utilizará para la prueba. Por defecto es 0.8.\n",
    "        new_img_width (int, optional): Ancho al que se redimensionarán las imágenes. Por defecto es 64.\n",
    "        new_img_height (int, optional): Altura a la que se redimensionarán las imágenes. Por defecto es 64.\n",
    "        batch_size (int, optional): Tamaño del lote para los generadores. Por defecto es 32.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Generadores de datos de imágenes para entrenamiento y prueba.\n",
    "\"\"\"\n",
    "def prepare_data_generators(cloudy_path, desert_path, green_path, water_path, base_dir='./data/', train_size=0.8, new_img_width=64, new_img_height=64, batch_size=32):\n",
    "    # Definir rutas para los datasets divididos\n",
    "    train_dir = os.path.join(base_dir, 'training_set')\n",
    "    test_dir = os.path.join(base_dir, 'test_set')\n",
    "\n",
    "    # Crear directorios para entrenamiento y prueba\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Lista de categorías y sus rutas correspondientes\n",
    "    categories = {\n",
    "        'cloudy': cloudy_path,\n",
    "        'desert': desert_path,\n",
    "        'green': green_path,\n",
    "        'water': water_path\n",
    "    }\n",
    "\n",
    "    # Dividir y copiar imágenes para cada categoría\n",
    "    for category, path in categories.items():\n",
    "        split_and_copy_images(category, path, train_dir, test_dir, train_size)\n",
    "\n",
    "    # Crear generadores de datos de imágenes para entrenamiento y prueba\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Flujo desde el directorio y redimensionamiento de las imágenes a (64, 64)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(new_img_width, new_img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(new_img_width, new_img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, test_generator\n",
    "\n",
    "# Dimensiones de las imágenes\n",
    "new_img_width=64\n",
    "new_img_height=64\n",
    "\n",
    "# Tamaño del lote\n",
    "batch_size = 32\n",
    "\n",
    "# Definir rutas de los datasets originales\n",
    "cloudy_path = \"./data/cloudy\"\n",
    "desert_path = \"data/desert\"\n",
    "green_path = \"data/green_area\"\n",
    "water_path = \"data/water\"\n",
    "\n",
    "# Uso de la función\n",
    "train_generator, test_generator = prepare_data_generators(cloudy_path, desert_path, green_path, water_path, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Define y compila un modelo CNN para la clasificación de imágenes.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple, optional): Forma de las imágenes de entrada. Por defecto es (64, 64, 3).\n",
    "        num_classes (int, optional): Número de clases de salida. Por defecto es 4.\n",
    "        learning_rate (float, optional): Tasa de aprendizaje para el optimizador. Por defecto es 0.00001.\n",
    "\n",
    "    Returns:\n",
    "        Model: Modelo compilado de Keras.\n",
    "\"\"\"\n",
    "def create_cnn_model(input_shape=(64, 64, 3), num_classes=4, learning_rate=0.00001):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Mostrar el resumen del modelo\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uso de la función\n",
    "model = create_cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_v2(input_shape=(64, 64, 3), num_classes=4, learning_rate=0.00001):\n",
    "    \"\"\"\n",
    "    Define y compila un segundo modelo CNN para la clasificación de imágenes.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple, optional): Forma de las imágenes de entrada. Por defecto es (64, 64, 3).\n",
    "        num_classes (int, optional): Número de clases de salida. Por defecto es 4.\n",
    "        learning_rate (float, optional): Tasa de aprendizaje para el optimizador. Por defecto es 0.00001.\n",
    "\n",
    "    Returns:\n",
    "        Model: Modelo compilado de Keras.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(1024, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', padding=\"same\")(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    custom_optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Mostrar el resumen del modelo\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uso de la función\n",
    "model_v2 = create_cnn_model_v2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para matrices de confusión y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Evalúa el modelo CNN utilizando el generador de datos de prueba, calcula varias métricas de rendimiento,\n",
    "    y genera una matriz de confusión visualmente atractiva.\n",
    "\n",
    "    Args:\n",
    "        model (Model): Modelo de Keras entrenado.\n",
    "        test_generator (DirectoryIterator): Generador de datos de prueba.\n",
    "        train_generator (DirectoryIterator): Generador de datos de entrenamiento (usado para obtener índices de clase).\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario que contiene la matriz de confusión, el informe de clasificación y métricas adicionales (precisión, recall, F1 score).\n",
    "\"\"\"\n",
    "def evaluate_and_plot_confusion_matrix(model, test_generator, train_generator):\n",
    "    \n",
    "    # Reiniciar el generador de prueba\n",
    "    test_generator.reset()\n",
    "    \n",
    "    # Realizar predicciones sobre el conjunto de prueba\n",
    "    predictions = model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n",
    "    \n",
    "    # Obtener las etiquetas predichas y verdaderas\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = test_generator.classes\n",
    "    \n",
    "    # Obtener los índices de clase y nombres de clase\n",
    "    class_indices = train_generator.class_indices\n",
    "    class_names = list(class_indices.keys())\n",
    "    \n",
    "    # Calcular la matriz de confusión\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Generar el heatmap de la matriz de confusión\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular el informe de clasificación\n",
    "    class_report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
    "    print('Informe de Clasificación')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Calcular métricas adicionales\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    print(f'Precisión: {accuracy}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    \n",
    "    # Retornar las métricas calculadas en un diccionario\n",
    "    return {\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Entrena el modelo CNN con los generadores de datos de entrenamiento y prueba.\n",
    "\n",
    "    Args:\n",
    "        model (Model): Modelo de Keras a entrenar.\n",
    "        train_generator (DirectoryIterator): Generador de datos de entrenamiento.\n",
    "        test_generator (DirectoryIterator): Generador de datos de prueba.\n",
    "        epochs (int, optional): Número de épocas para entrenar el modelo. Por defecto es 10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Historia del entrenamiento y tiempo total de entrenamiento en segundos.\n",
    "\"\"\"\n",
    "def train_cnn_model(model, train_generator, test_generator, epochs=10):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=test_generator\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    return history, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo 1 por 10 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "history, training_time = train_cnn_model(model, train_generator, test_generator, epochs=10)\n",
    "\n",
    "# Mostrar el tiempo total de entrenamiento\n",
    "print(f\"Tiempo total de entrenamiento: {training_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "evaluation_metrics = evaluate_and_plot_confusion_matrix(model, test_generator, train_generator)\n",
    "\n",
    "# Acceso a las métricas\n",
    "conf_matrix = evaluation_metrics['confusion_matrix']\n",
    "class_report = evaluation_metrics['classification_report']\n",
    "accuracy = evaluation_metrics['accuracy']\n",
    "precision = evaluation_metrics['precision']\n",
    "recall = evaluation_metrics['recall']\n",
    "f1 = evaluation_metrics['f1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo 1 por 50 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "history, training_time = train_cnn_model(model, train_generator, test_generator, epochs=50)\n",
    "\n",
    "# Mostrar el tiempo total de entrenamiento\n",
    "print(f\"Tiempo total de entrenamiento: {training_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "evaluation_metrics = evaluate_and_plot_confusion_matrix(model, test_generator, train_generator)\n",
    "\n",
    "# Acceso a las métricas\n",
    "conf_matrix = evaluation_metrics['confusion_matrix']\n",
    "class_report = evaluation_metrics['classification_report']\n",
    "accuracy = evaluation_metrics['accuracy']\n",
    "precision = evaluation_metrics['precision']\n",
    "recall = evaluation_metrics['recall']\n",
    "f1 = evaluation_metrics['f1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo 1 por 100 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "history, training_time = train_cnn_model(model, train_generator, test_generator, epochs=100)\n",
    "\n",
    "# Mostrar el tiempo total de entrenamiento\n",
    "print(f\"Tiempo total de entrenamiento: {training_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "evaluation_metrics = evaluate_and_plot_confusion_matrix(model, test_generator, train_generator)\n",
    "\n",
    "# Acceso a las métricas\n",
    "conf_matrix = evaluation_metrics['confusion_matrix']\n",
    "class_report = evaluation_metrics['classification_report']\n",
    "accuracy = evaluation_metrics['accuracy']\n",
    "precision = evaluation_metrics['precision']\n",
    "recall = evaluation_metrics['recall']\n",
    "f1 = evaluation_metrics['f1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo 2 por 10 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "history_v2, training_time_v2 = train_cnn_model(model_v2, train_generator, test_generator, epochs=10)\n",
    "\n",
    "# Mostrar el tiempo total de entrenamiento\n",
    "print(f\"Tiempo total de entrenamiento: {training_time_v2} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "evaluation_metrics = evaluate_and_plot_confusion_matrix(model_v2, test_generator, train_generator)\n",
    "\n",
    "# Acceso a las métricas\n",
    "conf_matrix = evaluation_metrics['confusion_matrix']\n",
    "class_report = evaluation_metrics['classification_report']\n",
    "accuracy = evaluation_metrics['accuracy']\n",
    "precision = evaluation_metrics['precision']\n",
    "recall = evaluation_metrics['recall']\n",
    "f1 = evaluation_metrics['f1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo 2 por 50 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "history_v2, training_time_v2 = train_cnn_model(model_v2, train_generator, test_generator, epochs=50)\n",
    "\n",
    "# Mostrar el tiempo total de entrenamiento\n",
    "print(f\"Tiempo total de entrenamiento: {training_time_v2} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "evaluation_metrics = evaluate_and_plot_confusion_matrix(model_v2, test_generator, train_generator)\n",
    "\n",
    "# Acceso a las métricas\n",
    "conf_matrix = evaluation_metrics['confusion_matrix']\n",
    "class_report = evaluation_metrics['classification_report']\n",
    "accuracy = evaluation_metrics['accuracy']\n",
    "precision = evaluation_metrics['precision']\n",
    "recall = evaluation_metrics['recall']\n",
    "f1 = evaluation_metrics['f1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo 2 por 100 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "history_v2, training_time_v2 = train_cnn_model(model_v2, train_generator, test_generator, epochs=100)\n",
    "\n",
    "# Mostrar el tiempo total de entrenamiento\n",
    "print(f\"Tiempo total de entrenamiento: {training_time_v2} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función\n",
    "evaluation_metrics = evaluate_and_plot_confusion_matrix(model_v2, test_generator, train_generator)\n",
    "\n",
    "# Acceso a las métricas\n",
    "conf_matrix = evaluation_metrics['confusion_matrix']\n",
    "class_report = evaluation_metrics['classification_report']\n",
    "accuracy = evaluation_metrics['accuracy']\n",
    "precision = evaluation_metrics['precision']\n",
    "recall = evaluation_metrics['recall']\n",
    "f1 = evaluation_metrics['f1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver ejemplos de instancias mal clasificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Muestra las imágenes mal clasificadas junto con sus etiquetas verdaderas y predichas.\n",
    "\n",
    "    Args:\n",
    "        model (Model): El modelo de Keras entrenado.\n",
    "        test_generator (DirectoryIterator): Generador de datos de prueba.\n",
    "        class_names (list): Lista de nombres de clases.\n",
    "        num_images (int): Número de imágenes mal clasificadas a mostrar. Por defecto son 20.\n",
    "\"\"\"\n",
    "def display_misclassified_images(model, test_generator, class_names, num_images=20):\n",
    "\n",
    "    # Reiniciar el generador de prueba\n",
    "    test_generator.reset()\n",
    "    \n",
    "    # Realizar predicciones sobre el conjunto de prueba\n",
    "    predictions = model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n",
    "    \n",
    "    # Obtener las etiquetas predichas y verdaderas\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = test_generator.classes\n",
    "    \n",
    "    # Encontrar índices de imágenes mal clasificadas\n",
    "    misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "    \n",
    "    # Mostrar algunas imágenes mal clasificadas\n",
    "    num_display = min(num_images, len(misclassified_indices))\n",
    "    plt.figure(figsize=(15, num_display * 2))\n",
    "    \n",
    "    for i, idx in enumerate(misclassified_indices[:num_display]):\n",
    "        # Obtener ruta de la imagen\n",
    "        img_path = test_generator.filepaths[idx]\n",
    "        \n",
    "        # Cargar la imagen\n",
    "        img = load_img(img_path, target_size=(test_generator.target_size))\n",
    "        img_array = img_to_array(img)\n",
    "        \n",
    "        # Mostrar la imagen\n",
    "        plt.subplot(num_display, 1, i + 1)\n",
    "        plt.imshow(img_array.astype(\"uint8\"))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Etiquetas verdadera y predicha\n",
    "        true_label = class_names[true_labels[idx]]\n",
    "        predicted_label = class_names[predicted_labels[idx]]\n",
    "        plt.title(f'True: {true_label} | Predicted: {predicted_label}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uso de la función\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "display_misclassified_images(model, test_generator, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
